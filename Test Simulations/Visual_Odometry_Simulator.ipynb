{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8ce58d",
   "metadata": {},
   "source": [
    "\n",
    "# Project Redspot – PeraHive  \n",
    "## Visual Odometry Simulator (Notebook Template)\n",
    "\n",
    "**Faculty of Engineering, University of Peradeniya**  \n",
    "**Topic:** Drone Localization using **Visual Odometry (VO)** (instead of Particle Filtering)\n",
    "\n",
    "---\n",
    "\n",
    "### What this notebook does\n",
    "This notebook simulates a drone moving on a **planar trajectory** and estimates its motion using a **feature-based Visual Odometry pipeline**:\n",
    "\n",
    "- Generate a synthetic “textured” scene (random dots)\n",
    "- Render camera frames along a known trajectory\n",
    "- Detect & match features between consecutive frames (ORB)\n",
    "- Estimate frame-to-frame motion (2D transform) using RANSAC\n",
    "- Integrate relative motion to get an estimated trajectory\n",
    "- Visualize ground-truth vs VO trajectory and error\n",
    "\n",
    "> This is a **simulation template** designed to mirror the layout of your existing `Simulator.ipynb`,\n",
    "> but using **Visual Odometry** instead of a **Particle Filter**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056363a",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "\n",
    "We will use:\n",
    "- `numpy` for math\n",
    "- `matplotlib` for plots\n",
    "- `opencv-python (cv2)` for ORB feature extraction + matching + RANSAC motion estimation\n",
    "\n",
    "If OpenCV is not available in your environment, install it via:\n",
    "```bash\n",
    "pip install opencv-python\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe704b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: OpenCV for feature-based VO\n",
    "try:\n",
    "    import cv2\n",
    "    OPENCV_OK = True\n",
    "    print(\"OpenCV detected:\", cv2.__version__)\n",
    "except Exception as e:\n",
    "    OPENCV_OK = False\n",
    "    print(\"OpenCV not available. Error:\", e)\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4debe79f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Background Theory\n",
    "\n",
    "### 1.1 Visual Odometry (VO)\n",
    "Visual Odometry estimates the **relative motion** of a camera by analyzing **consecutive frames**:\n",
    "\n",
    "- Detect visual features (corners/keypoints)\n",
    "- Match features between frame *t* and frame *t+1*\n",
    "- Estimate the camera motion that best explains the feature displacement\n",
    "- Integrate (accumulate) motion over time to form a trajectory\n",
    "\n",
    "### 1.2 Key difference from Particle Filter localization\n",
    "- Particle Filter + beacons: provides **absolute/global position** using a map & measurements\n",
    "- VO: provides **relative motion** (Δx, Δy, Δθ) and will **drift** over time unless corrected by landmarks, loop closure, or other sensors.\n",
    "\n",
    "### 1.3 What we estimate here\n",
    "To keep the simulator simple (and aligned with a 2D flight path), we estimate a **2D rigid transform** between frames:\n",
    "- Translation: Δx, Δy\n",
    "- Rotation: Δθ\n",
    "\n",
    "We do this using a robust estimator (RANSAC) over feature matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314095dd",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Simulation Design\n",
    "\n",
    "### 2.1 Synthetic scene\n",
    "We create a grayscale image with many random dots. This acts as a “textured floor / environment” so VO can track features.\n",
    "\n",
    "### 2.2 Camera frames along a trajectory\n",
    "We simulate a moving camera by applying:\n",
    "- a rotation\n",
    "- a translation (pixel shift)\n",
    "\n",
    "to the base texture.  \n",
    "This gives us a sequence of frames with known ground-truth motion.\n",
    "\n",
    "### 2.3 VO pipeline (feature-based)\n",
    "For each consecutive pair of frames:\n",
    "1. Detect ORB keypoints + descriptors\n",
    "2. Match descriptors (Hamming distance)\n",
    "3. Estimate 2D transform using RANSAC\n",
    "4. Convert that transform into Δx, Δy, Δθ\n",
    "5. Integrate to build the estimated trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_textured_scene(h=480, w=640, n_dots=2500, dot_radius=1):\n",
    "    \"\"\"Create a synthetic textured grayscale scene (random dots).\"\"\"\n",
    "    if not OPENCV_OK:\n",
    "        raise RuntimeError(\"OpenCV is required. Install with: pip install opencv-python\")\n",
    "    img = np.zeros((h, w), dtype=np.uint8)\n",
    "    ys = np.random.randint(0, h, size=n_dots)\n",
    "    xs = np.random.randint(0, w, size=n_dots)\n",
    "    for x, y in zip(xs, ys):\n",
    "        cv2.circle(img, (int(x), int(y)), dot_radius, 255, -1)\n",
    "    # Slight blur to create more stable features\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)\n",
    "    return img\n",
    "\n",
    "def warp_frame(base_img, dx_px, dy_px, dtheta_deg):\n",
    "    \"\"\"Warp base image by rotation + translation (simulated camera motion).\"\"\"\n",
    "    h, w = base_img.shape\n",
    "    center = (w/2, h/2)\n",
    "    M = cv2.getRotationMatrix2D(center, dtheta_deg, 1.0)  # 2x3\n",
    "    M[:, 2] += [dx_px, dy_px]\n",
    "    warped = cv2.warpAffine(\n",
    "        base_img, M, (w, h),\n",
    "        flags=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=0\n",
    "    )\n",
    "    return warped, M\n",
    "\n",
    "if not OPENCV_OK:\n",
    "    raise RuntimeError(\"OpenCV is required for this notebook. Install with: pip install opencv-python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be226c7",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Ground-truth Trajectory\n",
    "\n",
    "We define a smooth 2D path (x, y, θ) in **world units**.\n",
    "\n",
    "Then we map those motions to **pixel transforms** to generate frames.\n",
    "\n",
    "> In a real drone, pixels ↔ meters depends on camera intrinsics and scene depth.\n",
    "> Here we use a simple scale factor for demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_trajectory(num_steps=200):\n",
    "    \"\"\"Create a smooth planar trajectory. Returns x, y, theta (radians).\"\"\"\n",
    "    t = np.linspace(0, 2*np.pi, num_steps)\n",
    "    x = 2.5*np.cos(t) + 0.5*np.cos(3*t)\n",
    "    y = 2.0*np.sin(t) + 0.3*np.sin(2*t)\n",
    "    theta = 0.25*np.sin(1.5*t)  # small yaw oscillation\n",
    "    return x, y, theta\n",
    "\n",
    "num_steps = 220\n",
    "x_gt, y_gt, th_gt = generate_trajectory(num_steps)\n",
    "\n",
    "# Map world units to pixel motion (tunable)\n",
    "PX_PER_UNIT = 18.0\n",
    "DEG_PER_RAD = 180/np.pi\n",
    "\n",
    "dx_gt = np.diff(x_gt) * PX_PER_UNIT\n",
    "dy_gt = np.diff(y_gt) * PX_PER_UNIT\n",
    "dth_gt = np.diff(th_gt) * DEG_PER_RAD  # degrees\n",
    "\n",
    "dx_gt[:5], dy_gt[:5], dth_gt[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aad617",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Generate Synthetic Camera Frames\n",
    "\n",
    "We generate frames by applying the *cumulative* transform of the trajectory to the base texture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d804c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base = make_textured_scene()\n",
    "\n",
    "frames = []\n",
    "true_pose_px = [(0.0, 0.0, 0.0)]  # (x_px, y_px, theta_deg)\n",
    "\n",
    "x_px, y_px, th_deg = 0.0, 0.0, 0.0\n",
    "frames.append(base.copy())\n",
    "\n",
    "for i in range(num_steps - 1):\n",
    "    x_px += dx_gt[i]\n",
    "    y_px += dy_gt[i]\n",
    "    th_deg += dth_gt[i]\n",
    "    true_pose_px.append((x_px, y_px, th_deg))\n",
    "    f, _ = warp_frame(base, x_px, y_px, th_deg)\n",
    "    frames.append(f)\n",
    "\n",
    "# Show a few frames\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for idx, k in enumerate([0, (num_steps//2), num_steps-1]):\n",
    "    ax = fig.add_subplot(1, 3, idx+1)\n",
    "    ax.imshow(frames[k], cmap='gray')\n",
    "    ax.set_title(f\"Frame {k}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f7e9e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Visual Odometry Implementation (ORB + Matching + RANSAC)\n",
    "\n",
    "We estimate motion between consecutive frames using:\n",
    "- ORB features + binary descriptors\n",
    "- BFMatcher with Hamming distance\n",
    "- RANSAC to fit a 2D transform robustly (reject outliers)\n",
    "\n",
    "We extract:\n",
    "- Δx, Δy from translation\n",
    "- Δθ from rotation\n",
    "\n",
    "and integrate them to form the estimated trajectory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_motion_2d(frame_a, frame_b, max_features=1500):\n",
    "    \"\"\"\n",
    "    Estimate 2D motion between two frames using ORB + BFMatcher + RANSAC.\n",
    "    Returns: (success, dx, dy, dtheta_deg, inliers_ratio)\n",
    "    \"\"\"\n",
    "    orb = cv2.ORB_create(nfeatures=max_features)\n",
    "    kpa, desa = orb.detectAndCompute(frame_a, None)\n",
    "    kpb, desb = orb.detectAndCompute(frame_b, None)\n",
    "\n",
    "    if desa is None or desb is None or len(kpa) < 10 or len(kpb) < 10:\n",
    "        return False, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(desa, desb)\n",
    "    if len(matches) < 12:\n",
    "        return False, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    matches = sorted(matches, key=lambda m: m.distance)\n",
    "    keep = matches[: min(400, len(matches))]\n",
    "\n",
    "    pts_a = np.float32([kpa[m.queryIdx].pt for m in keep]).reshape(-1, 1, 2)\n",
    "    pts_b = np.float32([kpb[m.trainIdx].pt for m in keep]).reshape(-1, 1, 2)\n",
    "\n",
    "    M, inliers = cv2.estimateAffinePartial2D(\n",
    "        pts_a, pts_b,\n",
    "        method=cv2.RANSAC,\n",
    "        ransacReprojThreshold=3.0\n",
    "    )\n",
    "    if M is None or inliers is None:\n",
    "        return False, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    inliers_ratio = float(inliers.sum()) / float(len(inliers))\n",
    "\n",
    "    a, b, tx = M[0]\n",
    "    c, d, ty = M[1]\n",
    "    dtheta = np.degrees(np.arctan2(c, a))\n",
    "\n",
    "    return True, float(tx), float(ty), float(dtheta), inliers_ratio\n",
    "\n",
    "\n",
    "# Run VO over all frames\n",
    "dx_est, dy_est, dth_est, inlier_ratios = [], [], [], []\n",
    "\n",
    "for i in range(num_steps - 1):\n",
    "    ok, tx, ty, dtheta, r = estimate_motion_2d(frames[i], frames[i+1])\n",
    "    dx_est.append(tx)\n",
    "    dy_est.append(ty)\n",
    "    dth_est.append(dtheta)\n",
    "    inlier_ratios.append(r if ok else 0.0)\n",
    "\n",
    "dx_est = np.array(dx_est)\n",
    "dy_est = np.array(dy_est)\n",
    "dth_est = np.array(dth_est)\n",
    "inlier_ratios = np.array(inlier_ratios)\n",
    "\n",
    "dx_est[:5], dy_est[:5], dth_est[:5], inlier_ratios.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff369b3",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Integrate Estimated Motion to Reconstruct Trajectory\n",
    "\n",
    "We integrate:\n",
    "- x(t+1) = x(t) + Δx(t)\n",
    "- y(t+1) = y(t) + Δy(t)\n",
    "- θ(t+1) = θ(t) + Δθ(t)\n",
    "\n",
    "This produces the VO trajectory in the same **pixel-coordinate** space as the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_vo = [0.0]\n",
    "y_vo = [0.0]\n",
    "th_vo = [0.0]\n",
    "\n",
    "for i in range(num_steps - 1):\n",
    "    x_vo.append(x_vo[-1] + dx_est[i])\n",
    "    y_vo.append(y_vo[-1] + dy_est[i])\n",
    "    th_vo.append(th_vo[-1] + dth_est[i])\n",
    "\n",
    "x_vo = np.array(x_vo)\n",
    "y_vo = np.array(y_vo)\n",
    "th_vo = np.array(th_vo)\n",
    "\n",
    "x_true = np.array([p[0] for p in true_pose_px])\n",
    "y_true = np.array([p[1] for p in true_pose_px])\n",
    "th_true = np.array([p[2] for p in true_pose_px])\n",
    "\n",
    "pos_err = np.sqrt((x_vo - x_true)**2 + (y_vo - y_true)**2)\n",
    "\n",
    "pos_err[:5], pos_err[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6d1ca",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Results Visualization\n",
    "\n",
    "We plot:\n",
    "- Ground-truth trajectory vs VO-estimated trajectory\n",
    "- Position error over time\n",
    "- Inlier ratio (match quality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(x_true, y_true, label=\"Ground Truth\")\n",
    "plt.plot(x_vo, y_vo, label=\"Visual Odometry (Estimated)\")\n",
    "plt.xlabel(\"x (pixels)\")\n",
    "plt.ylabel(\"y (pixels)\")\n",
    "plt.title(\"Trajectory: Ground Truth vs Visual Odometry\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3.5))\n",
    "plt.plot(pos_err)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Position error (pixels)\")\n",
    "plt.title(\"VO Drift / Position Error Over Time\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3.5))\n",
    "plt.plot(inlier_ratios)\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"RANSAC inlier ratio\")\n",
    "plt.title(\"Match Quality Over Time (Higher is Better)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec27e1",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Discussion\n",
    "\n",
    "### 8.1 Why drift happens\n",
    "VO integrates small errors at every step, so even tiny biases accumulate into a large final error.\n",
    "\n",
    "### 8.2 How to improve realism and accuracy\n",
    "To make this closer to a real drone system:\n",
    "- Use camera intrinsics + 3D geometry (Essential matrix / PnP)\n",
    "- Add IMU fusion (Visual-Inertial Odometry, VIO)\n",
    "- Add loop closure (Visual SLAM)\n",
    "- Or keep IR beacons as occasional global corrections (best for GPS-denied)\n",
    "\n",
    "### 8.3 Linking back to your IR beacon project\n",
    "A strong final design often is:\n",
    "- VO/VIO for continuous motion\n",
    "- IR beacons for periodic absolute pose correction\n",
    "- Fuse in an EKF or pose-graph optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6b477",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Optional Extensions (Good for Phase 2)\n",
    "\n",
    "- Add noise (motion blur, lighting changes, dropped frames)\n",
    "- Compare ORB with other detectors (FAST+BRIEF, SIFT if available)\n",
    "- Use optical flow (Lucas–Kanade) instead of descriptor matching\n",
    "- Add a beacon-correction step to demonstrate drift reset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db35d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of notebook."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
